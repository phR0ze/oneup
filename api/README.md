# OneUp rust API

### Quick links
* [Overview](#overview)
  * [NixOS Dev Env](#nixos-dev-env)
  * [Configuration](#configuration)
* [API Contract](#api-contract)
* [Web Server](#web-server)
  * [Custom rejection](#custom-rejection)
* [Database](#database)
  * [Database UI](#database-ui)
  * [Database Model](#database-model)
  * [SQLx Migrations](#sqlx-migrations)
* [Security](#Security)
  * [API Security](#api-security)
  * [JWT Tokens](#jwt-tokens)
  * [Passwords](#passwords)
  * [CORS](#cors)
* [Testing](#testing)
  * [Unit tests](#unit-tests)

## Overview

### NixOS Dev Env
1. [Launch VSCode for Rust API](../README.md#vscode-for-rust-api)

2. Build and run the project by pressing `Ctrl+Shift+r`

3. From a shell you can curl the API
   ```bash
   $ curl -i localhost:8080/health
   ```

### Configuration
A combination of `dotenvy` to set environment variables based on a `.env` file and then `envy` to 
load any configuration from the environment variables into a `serde` deserialized struct provides a 
nice approach to configuration.

**Example**
```.env
# IP and Port to listen on
IP=0.0.0.0
PORT=8080
LOG_LEVEL=info

# Database URL
DATABASE_URL=sqlite://sqlite.db
```

### Errors

## API Contract
The API contract is a mechanism that allows for decoupling the frontend and backend development. The 
frontend can develop against the contract with mock endpoints while the backend is working on their 
portion to deliver what is in the contract. Swagger or OpenAPI is known for this contract-first 
approach.

**References**
* [Learn OpenAPI spec](https://learn.openapis.org/specification/paths)
* [Swagger 3.0](https://swagger.io/docs/specification/v3_0/basic-structure/)

### Swagger Spec
Using AI generate a [swagger spec](swagger.yaml) for the project

### Visualize the Swagger Spec
Using the [Swagger Editor](https://editor.swagger.io/) we can easily visualize the API based on the 
generated spec.

## Web Server
[Axum is my chosen web framework](https://github.com/phR0ze/tech-docs/tree/main/src/development/languages/rust/web/axum).
It provides a modern Tokio and Tower compatible service that is quite flexible and intuitive.

### Custom rejection
Axum needed a custom rejection for JSON payload parsting in order to get a consistent error response 
whether it was generated by an extractor early on or by application code during the handling of the 
request.

```rust
// Converts into an `errors::Error` in the extraction rejection path
#[derive(axum::extract::FromRequest)]
#[from_request(via(axum::Json), rejection(crate::errors::Error))]
pub struct Json<T>(T);

// Converts to `IntoResponse` in the positve extraction path
impl<T: serde::Serialize> axum::response::IntoResponse for Json<T> {
    fn into_response(self) -> axum::response::Response {
        let Self(value) = self;
        axum::Json(value).into_response()
    }
}
```

## Database
I'm going to use SQLx as my database driver and sqlite as the backend database implementation. SQLx 
has an excellent in memory sqlite implementation for testing and a file based implementation for 
persistent storage which is more than sufficient for anything except large applications.

### Database UI
I'm using the `sqlitebrowser` NixOS package for manual db viewing and modifications

### Database Model
I built an EER Diagram to model out the database using `mysql-workbench`.

Launch with:
```bash
$ nix-shell -p mysql-workbench
$ cd ~/Projects/oneup/api
$ mysql-workbench assets/db-model.mwb
```

* ***Rewards*** are points that have been cashed out as a reward e.g. actual cash or a prize
* ***Points*** a numerical value that can be associated with some kind of reward
* ***Actions*** allows for making a distinction between how the points were awarded
* ***Passwords*** stores the salt and hash of salted password to guarantee a unique hash

### SQLx Migrations
The following steps were used to integrate database migrations to run on every boot idempotently.

1. Install SQLx CLI
   ```bash
   $ cargo install sqlx-cli --no-default-features --features rustls,sqlite
   ```
2. Create a migration script and modify as desired
   ```bash
   $ sqlx migrate add -r create_schema
   ```
3. Apply migration in code
   ```bash
   $ sqlx::migrate!().run(&pool).await?;
   ```

## Security

### API Security
RESTful APIs should be stateless. They should not depend on sessions, but rather should come with 
some sort of authentication credential that must be validated on the server for every request.

**References**
* [REST API Security Essentials](https://restfulapi.net/security-essentials/)

* ***Always use HTTPS***
  * Authentication credentials can be as simply as a randomly generated access token, i.e. API Key, 
    when using HTTPS. The token can be delivered as a header bearer token.
* ***Use Password Hash***
  * Passwords must always be hashed to protect the system and minimize damage on compromise
* ***Never expose info in URLs***
  * Any PII data should not appear in URLs
* ***Consider OAuth2***
  * OAuth2 tokens are temporary and time bound unlike API Keys
  * OAuth2 allows for encoding information via JWTs such as profile or subscription acount info

### JWT Tokens
Using the [jsonwebtoken](https://github.com/Keats/jsonwebtoken) crate we can create our own JWTs.

* Create a new server secret and store it in the db on boot
* `1 hr` token duration in seconds
* secret used to sign the token

we can use [jwt,io](https://jwt.io) to test generated jwts

### Passwords
User passwords are concatenated with a random generated salt then hashed and both the salt the the 
final hash are stored for later validation by the system during login. This means that user passwords 
are never stored directly and have the extra layer of protection that the salt provides.

Additionally passwords can be created and deleted but never updated which removes a level of 
complexity and attack surface.

**References**
* [Salt and hash password with PBKDF2](https://rust-lang-nursery.github.io/rust-cookbook/cryptography/encryption.html)
* [Password hashing with PBKDF2](https://web3developer.io/password-hashing-with-pbkdf2-in-rust-using-ring/)

* `PBKDF2` is being used for password hashing which is designed to be slow down brute forcing

### CORS
By default, web browsers follow the ***Same-Origin Policy***, which only allows web pages to make 
requests to the same origin that served the page. While this is good for security, it's often 
limiting when modern, complex web apps need to fetch data from multiple domains. CORS is the 
technology that allows servers to relax the Same-Origin Policy, granting browsers permission to 
expose the response to frontend JavaScript code even when the origin is different.

## Testing
In order to consider the API stable enough for self-hosted running my goal is:
* Unit tests with 80% code coverage of API handlers
* Manual Post

### Unit tests
All the unit tests can be run with:
```bash
$ cd ~/Projects/oneup/api
$ cargo test
```

